
Showing logs for build [32mjoostvdg-environment-jx-aks-sta-8qsc2-12[0m stage [32mfrom-build-pack[0m and container [32mstep-credential-initializer-f7h7v[0m
{"level":"warn","ts":1569843452.795724,"logger":"fallback-logger","caller":"logging/config.go:69","msg":"Fetch GitHub commit ID from kodata failed: \"ref: refs/heads/master\" is not a valid GitHub commit ID"}
{"level":"info","ts":1569843452.7962399,"logger":"fallback-logger","caller":"creds-init/main.go:40","msg":"Credentials initialized."}

Showing logs for build [32mjoostvdg-environment-jx-aks-sta-8qsc2-12[0m stage [32mfrom-build-pack[0m and container [32mstep-working-dir-initializer-gw9vt[0m
{"level":"warn","ts":1569843455.8174512,"logger":"fallback-logger","caller":"logging/config.go:69","msg":"Fetch GitHub commit ID from kodata failed: open /var/run/ko/HEAD: no such file or directory"}
{"level":"info","ts":1569843455.8201005,"logger":"fallback-logger","caller":"bash/main.go:65","msg":"Successfully executed command \"mkdir -p /workspace/source /workspace/source/env\""}

Showing logs for build [32mjoostvdg-environment-jx-aks-sta-8qsc2-12[0m stage [32mfrom-build-pack[0m and container [32mstep-place-tools[0m

Showing logs for build [32mjoostvdg-environment-jx-aks-sta-8qsc2-12[0m stage [32mfrom-build-pack[0m and container [32mstep-git-source-joostvdg-environment-jx-aks-sta-hr4q9[0m
{"level":"warn","ts":1569843467.3740652,"logger":"fallback-logger","caller":"logging/config.go:69","msg":"Fetch GitHub commit ID from kodata failed: \"ref: refs/heads/master\" is not a valid GitHub commit ID"}
{"level":"info","ts":1569843468.507956,"logger":"fallback-logger","caller":"git/git.go:102","msg":"Successfully cloned https://github.com/********/environment-**-aks-staging.git @ v0.0.12 in path /workspace/source"}

Showing logs for build [32mjoostvdg-environment-jx-aks-sta-8qsc2-12[0m stage [32mfrom-build-pack[0m and container [32mstep-git-merge[0m
Using SHAs from PULL_REFS=master:9e765541a8c5791a82558e60716e02701f7fcdf1
WARNING: no SHAs to merge, falling back to initial cloned commit

Showing logs for build [32mjoostvdg-environment-jx-aks-sta-8qsc2-12[0m stage [32mfrom-build-pack[0m and container [32mstep-setup-jx-git-credentials[0m
Generated Git credentials file /builder/home/git/credentials

Showing logs for build [32mjoostvdg-environment-jx-aks-sta-8qsc2-12[0m stage [32mfrom-build-pack[0m and container [32mstep-build-helm-apply[0m
Modified file /workspace/source/env/Chart.yaml to set the chart to version 12
Copying the helm source directory /workspace/source/env to a temporary location for building and applying /tmp/**-helm-apply-997405937/env
Applying helm chart at /tmp/**-helm-apply-997405937/env as release name ** to namespace **-staging
verifying the helm requirements versions in dir: . using version stream URL:  and git ref: 
Deleting and cloning the Jenkins X versions repo
Cloning the Jenkins X versions repo https://github.com/jenkins-x/jenkins-x-versions.git with ref refs/heads/master to /builder/home/.**/jenkins-x-versions
Ignoring templates/.gitignore
Ignoring templates/certificate.yaml
Ignoring templates/issuer.yaml
Wrote chart values.yaml /tmp/**-helm-apply-997405937/env/values.yaml generated from directory tree
generated helm /tmp/**-helm-apply-997405937/env/values.yaml

PipelineSecrets:
  DockerConfig: '{"auths":{"acctestaks1.azurecr.io":{"auth":"YWNjdGVzdGFrczE6MzVab00rWURreVk1SGpyalBOM3Y0U0t1cEIxVTVETDI="}}}'
cbcore:
  OperationsCenter:
    CSRF:
      ProxyCompatibility: true
    HostName: cbcore.staging.**-aks.kearos.net
    Image:
      dockerImage: cloudbees/cloudbees-cloud-core-oc:2.176.3.3
    Ingress:
      Annotations:
        kubernetes.io/ingress.class: nginx
        kubernetes.io/tls-acme: "true"
        nginx.ingress.kubernetes.io/proxy-body-size: 50m
        nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
      tls:
        Enable: true
        Host: cbcore.staging.**-aks.kearos.net
        SecretName: tls-staging-**-aks-kearos-net-p
    ServiceType: ClusterIP
  nginx-ingress:
    Enabled: false
cleanup:
  Annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: hook-succeeded
  Args:
  - --cleanup
controllerbuild:
  enabled: true
controllerworkflow:
  enabled: false
dockerRegistry: acctestaks1.azurecr.io
expose:
  Annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
  Args:
  - --v
  - 4
  config:
    domain: staging.**-aks.kearos.net
    exposer: Ingress
    http: "false"
    tlsacme: "true"
grafana:
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - disableDeletion: true
        editable: true
        folder: default
        name: Default
        options:
          path: /var/lib/grafana/dashboards/default
        orgId: 1
        type: file
  dashboards:
    default:
      Capacity:
        datasource: Prometheus
        gnetId: 5228
        revision: 6
      Costs:
        datasource: Prometheus
        gnetId: 8670
        revision: 1
      Costs-Pod:
        datasource: Prometheus
        gnetId: 6879
        revision: 1
      Deployments:
        datasource: Prometheus
        gnetId: 8588
        revision: 1
      Jenkins-OLD:
        datasource: Prometheus
        gnetId: 9964
        revision: 1
      Summary:
        datasource: Prometheus
        gnetId: 8685
        revision: 1
      Volumes:
        datasource: Prometheus
        gnetId: 6739
        revision: 1
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - access: proxy
        isDefault: true
        name: Prometheus
        type: prometheus
        url: http://**-prom-server
  ingress:
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
    enabled: true
    hosts:
    - grafana.staging.**-aks.kearos.net
    tls:
    - hosts:
      - grafana.staging.**-aks.kearos.net
      secretName: tls-staging-**-aks-kearos-net-p
jenkins:
  Servers:
    Global:
      EnvVars:
        TILLER_NAMESPACE: kube-system
  enabled: false
keycloak:
  keycloak:
    image:
      tag: 7.0.0
    ingress:
      annotations:
        kubernetes.io/ingress.class: nginx
        kubernetes.io/tls-acme: "true"
      enabled: true
      hosts:
      - keycloak.staging.**-aks.kearos.net
      tls:
      - hosts:
        - keycloak.staging.**-aks.kearos.net
        secretName: tls-staging-**-aks-kearos-net-p
prom:
  alertmanager:
    ingress:
      enabled: false
    resources:
      limits:
        cpu: 10m
        memory: 20Mi
      requests:
        cpu: 5m
        memory: 10Mi
  alertmanagerFiles:
    alertmanager.yml:
      global: {}
      receivers:
      - name: default
        slack_configs:
        - api_url: https://hooks.slack.com/services/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
          channel: '#notify'
          send_resolved: true
          text: '{{ .CommonAnnotations.description }} {{ .CommonLabels.app_kubernetes_io_instance}} '
          title: '{{ .CommonAnnotations.summary }} '
          title_link: http://my-prometheus.com/alerts
          username: Alertmanager
      route:
        group_by:
        - alertname
        - app_kubernetes_io_instance
        receiver: default
  kubeStateMetrics:
    resources:
      limits:
        cpu: 10m
        memory: 50Mi
      requests:
        cpu: 5m
        memory: 25Mi
  nodeExporter:
    resources:
      limits:
        cpu: 10m
        memory: 20Mi
      requests:
        cpu: 5m
        memory: 10Mi
  pushgateway:
    resources:
      limits:
        cpu: 10m
        memory: 20Mi
      requests:
        cpu: 5m
        memory: 10Mi
  server:
    ingress:
      enabled: false
    resources:
      limits:
        cpu: 100m
        memory: 1000Mi
      requests:
        cpu: 10m
        memory: 500Mi
  serverFiles:
    alerts:
      groups:
      - name: jobs
        rules:
        - alert: JenkinsTooManyJobsQueued
          annotations:
            description: '{{ $labels.app_kubernetes_io_instance }} has {{ $value }}
              jobs stuck in the queue'
            summary: ' {{ $labels.app_kubernetes_io_instance }} too many jobs queued'
          expr: sum(jenkins_queue_size_value) > 5
          for: 1m
          labels:
            severity: notify
        - alert: JenkinsTooManyJobsStuckInQueue
          annotations:
            description: ' {{ $labels.app_kubernetes_io_instance }} has {{ $value
              }} jobs in queue'
            summary: ' {{ $labels.app_kubernetes_io_instance }} too many jobs queued'
          expr: sum(jenkins_queue_stuck_value) by (app_kubernetes_io_instance) > 5
          for: 1m
          labels:
            severity: notify
        - alert: JenkinsWaitingTooMuchOnJobStart
          annotations:
            description: '{{ $labels.app_kubernetes_io_instance }} is waiting on average
              {{ $value }} seconds to start a job'
            summary: '{{ $labels.app_kubernetes_io_instance }} waits too long for
              jobs'
          expr: sum (jenkins_job_waiting_duration) by (app_kubernetes_io_instance)
            > 0.05
          for: 1m
          labels:
            severity: notify
        - alert: JenkinsTooLowJobSuccessRate
          annotations:
            description: ' {{ $labels.app_kubernetes_io_instance }} instance has {{
              $value }}% of jobs being successful'
            summary: ' {{ $labels.app_kubernetes_io_instance }} has a too low job
              success rate'
          expr: sum(jenkins_runs_success_total) by (app_kubernetes_io_instance) /
            sum(jenkins_runs_total_total) by (app_kubernetes_io_instance) < 0.60
          for: 1m
          labels:
            severity: notify
      - name: uptime
        rules:
        - alert: JenkinsNewOrRestarted
          annotations:
            description: ' {{ $labels.app_kubernetes_io_instance }} has low uptime
              and was either restarted or is a new instance (uptime: {{ $value }}
              hours)'
            summary: ' {{ $labels.app_kubernetes_io_instance }} has low uptime'
          expr: sum(vm_uptime_milliseconds) by (app_kubernetes_io_instance) / 3600000
            < 2
          for: 3m
          labels:
            severity: notify
      - name: plugins
        rules:
        - alert: JenkinsTooManyPluginsNeedUpate
          annotations:
            description: ' {{ $labels.app_kubernetes_io_instance }} has {{ $value
              }} plugins that require an update'
            summary: ' {{ $labels.app_kubernetes_io_instance }} too many plugins updates'
          expr: sum(jenkins_plugins_withUpdate) by (app_kubernetes_io_instance) >
            3
          for: 1m
          labels:
            severity: notify
      - name: jvm
        rules:
        - alert: JenkinsToManyOpenFiles
          annotations:
            description: ' {{ $labels.app_kubernetes_io_instance }} instance has used
              {{ $value }} of available open files'
            summary: ' {{ $labels.app_kubernetes_io_instance }} has a to many open
              files'
          expr: sum(vm_file_descriptor_ratio) by (app_kubernetes_io_instance) > 0.040
          for: 5m
          labels:
            severity: notify
        - alert: JenkinsVMMemoryRationTooHigh
          annotations:
            description: '{{$labels.app_kubernetes_io_instance}} has a too high VM
              memory ration'
            summary: '{{$labels.app_kubernetes_io_instance}} too high memory ration'
          expr: sum(vm_memory_heap_usage) by (app_kubernetes_io_instance) > 0.70
          for: 3m
          labels:
            severity: notify
        - alert: JenkinsTooManyPluginsNeedUpate
          annotations:
            description: '{{ $labels.instance }} has too low Garbage Collection throughput'
            summary: '{{ $labels.instance }} too low GC throughput'
          expr: 1 - sum(vm_gc_G1_Young_Generation_time)by (app_kubernetes_io_instance)  /  sum
            (vm_uptime_milliseconds) by (app_kubernetes_io_instance) < 0.99
          for: 30m
          labels:
            severity: notify
      - name: web
        rules:
        - alert: JenkinsTooSlow
          annotations:
            description: '{{ $labels.app_kubernetes_io_instance }}  More then 1% of
              requests are slower than 1s (request time: {{ $value }})'
            summary: '{{ $labels.app_kubernetes_io_instance }} is too slow'
          expr: sum(http_requests{quantile="0.99"} ) by (app_kubernetes_io_instance)
            > 1
          for: 3m
          labels:
            severity: notify
        - alert: AppTooSlow
          annotations:
            description: ' {{ $labels.ingress }} - More then 5% of requests are slower
              than 0.25s'
            summary: Application - {{ $labels.ingress }} - is too slow
          expr: sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le="0.25"}[5m]))
            by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m]))
            by (ingress) < 0.95
          for: 5m
          labels:
            severity: notify
      - name: healthcheck
        rules:
        - alert: JenkinsHealthScoreToLow
          annotations:
            description: ' {{ $labels.app_kubernetes_io_instance }} a health score
              lower than 100%'
            summary: ' {{ $labels.app_kubernetes_io_instance }} has a to low health
              score'
          expr: sum(jenkins_health_check_score) by (app_kubernetes_io_instance) <
            1
          for: 5m
          labels:
            severity: notify
        - alert: JenkinsTooSlowHealthCheck
          annotations:
            description: ' {{ $labels.app_kubernetes_io_instance }} is responding
              too slow to the regular health check'
            summary: ' {{ $labels.app_kubernetes_io_instance }} responds too slow
              to health check'
          expr: sum(jenkins_health_check_duration{quantile="0.999"}) by (app_kubernetes_io_instance)
            > 0.001
          for: 1m
          labels:
            severity: notify
      - name: nodes
        rules:
        - alert: JenkinsTooManyOfflineNodes
          annotations:
            description: '{{ $labels.app_kubernetes_io_instance }} has {{ $value }}
              nodes that are offline for some time (5 minutes)'
            summary: '{{ $labels.app_kubernetes_io_instance }} has a too many offline
              nodes'
          expr: sum(jenkins_node_offline_value) by (app_kubernetes_io_instance) >
            3
          for: 1m
          labels:
            severity: notify
prow: {}

Using values files: /tmp/**-helm-apply-997405937/env/values.yaml
WARNING: No $CHART_REPOSITORY defined so using the default value of: http://jenkins-x-chartmuseum:8080
Adding missing Helm repo: storage.googleapis.com https://storage.googleapis.com/chartmuseum.jenkins-x.io
Successfully added Helm repository storage.googleapis.com.
Adding missing Helm repo: jenkins-x-chartmuseum http://jenkins-x-chartmuseum:8080
Successfully added Helm repository jenkins-x-chartmuseum.
error: failed to add chart repositories: failed to load the Helm requirements file: error converting YAML to JSON: yaml: line 31: could not find expected ':'
[31m
Pipeline failed on stage 'from-build-pack' : container 'step-build-helm-apply'. The execution of the pipeline has stopped.[0m
